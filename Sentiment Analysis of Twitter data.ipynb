{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from twitter import Twitter\n",
    "from twitter import oauth_dance, OAuth\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONSUMER_KEY = 'QdrOrGTx0JqAzTSAZkeF8ZCxT'\n",
    "CONSUMER_SECRET = '5TNSrWNfOq2ek55HMAjKbgmd24NSUY8fbWhmZeHNyQ6ELVEl4b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dance_with_twitter():\n",
    "    # 'QdrOrGTx0JqAzTSAZkeF8ZCxT',\n",
    "    # '5TNSrWNfOq2ek55HMAjKbgmd24NSUY8fbWhmZeHNyQ6ELVEl4b',\n",
    "    # '1606571954-nQrQ4iQaOku7NcABsB44y1bp1ukSkYGumZNMeE8',\n",
    "    # 'dzMLIjhcnABo3fCJITbyOhoNh84eOM6Kh289bilVxSXx2'\n",
    "    oauth_token, oauth_token_secret = oauth_dance(\"Test Bot\", CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    return oauth_token, oauth_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connect_to_twiter(oauth_token, oauth_token_secret):\n",
    "    try:\n",
    "        t = Twitter(auth=OAuth(oauth_token, oauth_token_secret, CONSUMER_KEY, CONSUMER_SECRET))\n",
    "    except:\n",
    "        print(\"can't connect to twitter. bummer. frowny face.\")\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_twitter(t, subject):\n",
    "    tweets = t.search.tweets(q='#' + subject)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! We're gonna get you all set up to use Test Bot.\n",
      "\n",
      "In the web browser window that opens please choose to Allow\n",
      "access. Copy the PIN number that appears on the next page and paste or\n",
      "type it here:\n",
      "\n",
      "Opening: https://api.twitter.com/oauth/authorize?oauth_token=DpQuugAAAAAAiuD4AAABUzHF0WI\n",
      "\n",
      "Please enter the PIN: 6989830\n"
     ]
    }
   ],
   "source": [
    "oauth_token, oauth_token_secret = dance_with_twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = connect_to_twiter(oauth_token, oauth_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = t.search.tweets(q='#koala #culling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets['statuses'])\n",
    "for i in range(len(tweets['statuses'])):\n",
    "    print tweets['statuses'][i]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data from other Twitter scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pandas.read_csv('cull.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught between a rock & a hard place, #Australia needs to cull overabundant\" #koala colony to save #environment : http:// ow.ly/3xVtW5\"\n"
     ]
    }
   ],
   "source": [
    "print tweets['text'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_with_data(folder):\n",
    "    movie_reviews_data_folder = folder\n",
    "    dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
    "    print(\"n_samples: %d\" % len(dataset.data))\n",
    "\n",
    "    # split the dataset in training and test set:\n",
    "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
    "\n",
    "    # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "    # that are too rare or too frequent\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
    "        ('clf', LinearSVC(C=1000)),\n",
    "    ])\n",
    "\n",
    "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "    # more useful.\n",
    "    # Fit the pipeline on the training set using grid search for the parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    }\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "    grid_search = grid_search.fit(docs_train, y_train)\n",
    "    \n",
    "    # TASK: print the cross-validated scores for the each parameters set\n",
    "    # explored by the grid search\n",
    "    print(grid_search.grid_scores_)\n",
    "\n",
    "    # TASK: Predict the outcome on the testing set and store it in a variable\n",
    "    # named y_predicted\n",
    "    y_predicted = grid_search.predict(docs_test)\n",
    "\n",
    "    # Print the classification report\n",
    "    print(metrics.classification_report(y_test, y_predicted,\n",
    "                                        target_names=dataset.target_names))\n",
    "\n",
    "    # Print and plot the confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "    print(cm)\n",
    "\n",
    "    #plt.matshow(cm)\n",
    "    #plt.show()\n",
    "    \n",
    "    return dataset, grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n",
      "[mean: 0.82867, std: 0.01789, params: {'vect__ngram_range': (1, 1)}, mean: 0.84467, std: 0.01088, params: {'vect__ngram_range': (1, 2)}]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.85      0.89      0.87       243\n",
      "        pos       0.89      0.86      0.87       257\n",
      "\n",
      "avg / total       0.87      0.87      0.87       500\n",
      "\n",
      "[[217  26]\n",
      " [ 37 220]]\n"
     ]
    }
   ],
   "source": [
    "dataset, grid_search = train_with_data('txt_sentoken/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:3: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:4: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('neg', 'Koala cull suggested to save native marsupials from chlamydia http:// ift.tt/1R8ggaP')\n",
      "('neg', \"Hey guys, there's a koala cull in Australia . http://www. abc.net.au/news/2015-10-0 6/health-of-great-ocean-road-koala-population-improving/6829156 \")\n",
      "('pos', \"Australia begins koala cull http:// a.msn.com/01/en-au/AAeSV Ld?ocid=st Witness the truly phenomenal problem-solving abilities of the 'great' human brain.\")\n",
      "('neg', 'Australia begins koala cull to save species from mass starvation http:// dlvr.it/CHjhK2 pic.twitter.com/3GX9WJ4Zg3')\n",
      "('neg', 'Koala overpopulation forces Australia to start largest cull , relocation plan http:// xhne.ws/Jwefc (AFP pics) pic.twitter.com/Tsqn159wlh')\n",
      "('pos', 'Australia mulls koala cull , Japans whaling advocates eat up the irony like delicious whalemeat http:// en.rocketnews24.com/2015/05/28/aus tralia-mulls-koala-cull-japans-whaling-advocates-eat-up-the-irony-like-delicious-whale-meat/ via @RocketNews24En')\n",
      "('neg', 'Koala cull underway at Cape Otway http:// au.tusueldo.com/tVf #news #Australia')\n",
      "('pos', 'South Australia koala cull not on horizon: Ian Hunter http://www. theaustralian.com.au/national-affai rs/state-politics/south-australia-koala-cull-not-on-horizon-ian-hunter/story-e6frgczx-1227448234752 ')\n",
      "('neg', 'South Australia koala cull not on horizon: Ian Hunter... http:// fb.me/2kNW5LR6j')\n",
      "('pos', '#Koala cull resumes in Australian tourist destination http:// ln.is/focusingonwild life.com/3zFpw via @Wildlife_Focus #Australia #StopTheCull')\n",
      "('neg', 'Caught between a rock & a hard place, #Australia needs to cull overabundant\" #koala colony to save #environment : http:// ow.ly/3xVtW5\"')\n",
      "('pos', 'Australia To Kill Koalas? Ecological Catastrophe Sparks Cull : The koala may be one of the most iconic Austra... http:// binged.it/1JfjgAC')\n",
      "('neg', 'Overcrowded koala colony faces cull : A colony of koalas in Cape Otway, southern Australia , may have to be cull ... http:// bit.ly/1G6yPeP')\n",
      "('pos', 'Koala colony faces cull as food supply dwindles: An overcrowded colony of about 1,000 koalas in Australia is b... http:// bit.ly/1Rs2L72')\n",
      "('neg', 'Australia planning a koala cull . Are they mad? http://www. timeslive.co.za/scitech/2015/0 5/27/Australia-could-kill-over-crowded-koalas ')\n",
      "('pos', \"Fears of another koala cull in Australia as officials claim they are 'overabundant' in Victoria http://www. snsanalytics.com/9Lymy4\")\n",
      "('pos', 'Australia mulls koala cull , Japans whaling advocates eat up the irony like delicious whale meat http:// ift.tt/1HM2RSs (RocketNews24)')\n",
      "('neg', 'Australia mulls koala cull , Japans whaling advocates eat up the irony like delicious whalemeat http:// wp.me/p2nZlN-10Tu')\n",
      "('neg', 'Wowblack eye month for Australia first Pistol and Boo almost got whacked Down Under, rumoured Koala Bear cull and https:// subprimecanada.wordpress.com/2015/05/27/wow -black-eye-month-for-australia-first-pistol-and-boo-almost-got-whacked-down-under-rumoured-koala-bear-cull-and-now-the-gainfully-employe-d-of-australias-major-cities-get-real-familiar-with-t ')\n",
      "('neg', 'Overpopulated koala colony in Australia may lead to a cull , worrying animal lobbyists http:// tdy.sg/1J4TZc7 pic.twitter.com/pADy8MyVbT')\n",
      "('pos', \"Fears of koala cull in crowded #Australia 'n colony http:// jenke.rs/e3SAg6 pic.twitter.com/Foyp3TtY7H\")\n",
      "('pos', 'Koala cull in Australia prompts call for long-term solution | Toronto Star http:// fb.me/1Hvly3O83')\n",
      "('pos', 'Fears of koala cull in crowded #Australia colony http:// bit.ly/1BovIaA pic.twitter.com/7CPxsg25B4')\n",
      "('pos', '#Australia Koala cull resumes in Australian tourist destination: national symbol, but authorities in Austra... http:// tinyurl.com/mndlkcv')\n",
      "('pos', \"Fears of another koala cull in Australia as officials claim they are 'overabundant' in Victoria http://www. independent.co.uk/news/world/aus tralasia/fears-of-another-koala-cull-in-australia-as-officials-claim-they-are-overabundant-in-victoria-10275391.html \")\n",
      "('pos', \"Fears of another #koala cull in #Australia as officials claim they are 'overabundant' http:// goo.gl/xBif3p pic.twitter.com/jbJX0TWjnB\")\n",
      "('neg', 'Australia is planning to cull koala again @karluisq')\n",
      "('pos', 'Second cull imminent at one of the biggest koala colonies in Australia . http:// bit.ly/1LCKTDi pic.twitter.com/3GOR9MKW8L')\n",
      "('neg', 'Raw: Contentious Koala Cull in Australia : https:// youtu.be/p2EU6uDvr70 via @YouTube')\n",
      "('neg', 'AP News Minute: US military advisors train Iraqi troops in fight against ISIS; Koala cull revealed in Australia ...-> http:// bit.ly/1dJMC7V')\n",
      "('neg', 'Raw: Contentious Koala Cull in Australia http:// ift.tt/1FXklLX #AssociatedPress #Associated #Press #news')\n",
      "('pos', 'Raw: Contentious Koala Cull in Australia : http:// ln.is/www.youtube.co m/xjGeO @YouTube')\n",
      "('pos', '#streaming Raw: Contentious Koala Cull in Australia : Raw: Contentious Koala Cull in Australi... http:// ln.is/www.youtube.co m/ZZzEG Freelancesystem')\n",
      "('pos', 'Raw: Contentious Koala Cull in Australia | Watch: http:// ift.tt/1FXklLX via AP')\n",
      "('neg', '#Raw : #Contentious #Koala #Cull in #Australia http:// wp.me/p5wiVg-5xf pic.twitter.com/mWWbLewU3l')\n",
      "('pos', 'Video: AP - Raw: Contentious Koala Cull in Australia : Between 2012 and 2013 the Victorian state government ca... http:// bit.ly/1FXhZfX')\n",
      "('neg', 'Why did they cull koala bears in Australia Id have had one at least!')\n",
      "('pos', 'domdyer70: Disgraceful Koala cull generates huge anger in Australia & around the world http://www. independent.co.uk/news/world/aus tralasia/almost-700-koalas-secretly-culled-in-australia-because-they-were-just-dying-anyway-10090043.html ')\n",
      "('pos', 'Disgraceful Koala cull generates huge anger in Australia & around the world http://www. independent.co.uk/news/world/aus tralasia/almost-700-koalas-secretly-culled-in-australia-because-they-were-just-dying-anyway-10090043.html ')\n",
      "('neg', 'Press Statement Cee4life - Secret Koala Cull , Victoria, AUSTRALIA ... http:// fb.me/1HzvfDdB2')\n",
      "('pos', \"Russia hopes Vladimir Putin's Brisbane G20 koala friend survived mass cull : Russian embassy in Australia ... http:// bit.ly/1GUW5IS #AAR\")\n",
      "('neg', 'A #koala cull is not the solution; we must look at root causes of the problem http:// ow.ly/JVIhF article via @guardiannews #Australia')\n",
      "('neg', \"#Russia 's Vladimir #Putin says he hopes the #koala he cuddled during his G20 visit to #Australia wasn't a victim of a recent cull #aaptravel\")\n",
      "('pos', 'Secret Koala Cull , AUSTRALIA - Someone in the Australian Government approved a Koala cull along the Great Ocean... http:// fb.me/4rj12DXVk')\n",
      "('neg', '#rt Why Australia is killing koala bears http:// a.msn.com/01/en-ca/BBide GK BULL! Dying from HUMAN encroachment and big biz profits!! CULL HUMANS!')\n",
      "('neg', 'Koala covert cull in Australia by authorities. Was there a question to the cause or only band-aid the symptom? http:// rt.com/news/237585-ko alas-killed-australia-culled/ ')\n",
      "('neg', 'Australia : Hundreds of koalas killed in secret cull , officials admit http:// thetim.es/17Qh7wi (Getty) pic.twitter.com/uqXY8o6fb6 #koala #Australia')\n",
      "('neg', 'Koala cull : 700 endangered marsupials secretly slain by Australian authorities #Australia #Koalas http:// fb.me/1WcIgp7N4')\n",
      "('neg', '#Koala cull : 700 endangered marsupials secretly slain by #Australia authorities - http:// rt.com/news/237585-ko alas-killed-australia-culled/ Less than 100,000 left. Bizarre!')\n",
      "('neg', 'Australia : Victoria Koala Cull http:// wp.me/pkkD6-1C6')\n",
      "('neg', 'Koala cull Story. For those asking me endless questions instead of using google to search. #Australia http://www. 9news.com.au/national/2015/ 03/04/07/29/almost-700-koalas-killed-in-secret-cull-of-victorian-colony ')\n",
      "('neg', \"Wow, having visited wildlife sanctuaries in Australia I'm stunned to read of this koala cull http://www. theaustralian.com.au/national-affai rs/state-politics/hundreds-of-victorian-koalas-killed-off-in-secret-cull/story-e6frgczx-1227247003919?sv=3847fdec00cf4739b7e8653da33072ea \")\n",
      "('neg', \"MT @AFP : Controversial koala cull in Australia : 'The whole of the cape smelled of dead koalas' http:// u.afp.com/VSk pic.twitter.com/HNrSgneqCW\")\n",
      "('pos', \"A secret cull of 700 cuddly Koala 's is causing controversy in Australia today http:// ab.co/1B5GeVZ #koalagate pic.twitter.com/jGE8ayxK5L\")\n",
      "('neg', \"roo cull , koala cull , croc cull , etc etc RT @_struct : If I was Japan, I'd take Australia to the ICJ to stop the shark cull .\")\n",
      "('neg', ' @planetjedward : Koala Bears in Australia Love Jedward Hair! http:// twitpic.com/ckzzni \" that\\'s. Because your hair is awesome! :D #JedwardHair\"')\n",
      "('neg', \"A firefighter giving water to a young Koala Bear during bush fires in Australia pic.twitter.com/EaHTRl5jNP @ChrisGPackham Aussies don't cull !!\")\n"
     ]
    }
   ],
   "source": [
    "with open(\"sentiment_analysis_of_koala_cull_based_on_movies_training_set.csv\", 'w+') as f:\n",
    "    for tweet in tweets['text']:\n",
    "        f.write(dataset.target_names[grid_search.predict([str(tweet)])] + ';' + str(tweet) + \"\\n\")\n",
    "        print(dataset.target_names[grid_search.predict([str(tweet)])], str(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(tweets['text'])\n",
    "X_train_counts.shape\n",
    "count_vect.vocabulary_.get(u'algorithm')\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 470)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-084213a2707d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"take2.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m';'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/metaestimators.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \"\"\"\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/utils/metaestimators.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'The tfidf vector is not fitted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 238\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "with open(\"take2.csv\", 'w+') as f:\n",
    "    for tweet in tweets['text']:\n",
    "        f.write(dataset.target_names[grid_search.predict(X_train_tfidf)] + ';' + str(tweet) + \"\\n\")\n",
    "        print(dataset.target_names[grid_search.predict([str(tweet)])], str(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 46)\t0.259096428837\n",
      "  (0, 233)\t0.259096428837\n",
      "  (0, 328)\t0.259096428837\n",
      "  (0, 343)\t0.259096428837\n",
      "  (0, 298)\t0.235041598201\n",
      "  (0, 203)\t0.217974424253\n",
      "  (0, 300)\t0.122492086491\n",
      "  (0, 212)\t0.259096428837\n",
      "  (0, 6)\t0.259096428837\n",
      "  (0, 33)\t0.193919593617\n",
      "  (0, 295)\t0.163614091075\n",
      "  (0, 86)\t0.184774371447\n",
      "  (0, 294)\t0.259096428837\n",
      "  (0, 56)\t0.259096428837\n",
      "  (0, 460)\t0.152797589032\n",
      "  (0, 91)\t0.0603583035194\n",
      "  (0, 234)\t0.0892920336846\n",
      "  (0, 399)\t0.235041598201\n",
      "  (0, 205)\t0.259096428837\n",
      "  (0, 213)\t0.259096428837\n",
      "  (0, 219)\t0.0669569325049\n",
      "  (0, 146)\t0.0603583035194\n",
      "  (0, 257)\t0.11865302417\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
